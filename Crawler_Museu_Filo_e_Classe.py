# -*- coding: utf-8 -*-
"""CrawlerMuseu_Filo_e_Classe

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15Y3ac3S8td4hFGOgcsKFxaeiRcw7lW-5
"""

from google.colab import drive
drive.mount('/content/drive')

import requests
from bs4 import BeautifulSoup
import csv
import os
from datetime import date
import pandas as pd

def Cria_Diretorios():

  os.chdir("/content/drive/MyDrive/Projeto_IC_Unicamp/dados")
  print("Pasta atual: " + os.getcwd())

  if os.path.exists("Coletas") == True:
    print("Pasta Principal já existe")

  else:
    os.mkdir("Coletas")
    print("Pasta principal criada")

  os.chdir("Coletas")

  data_atual = "eae"

  if os.path.exists("Coleta_"+data_atual) == True:
    print("Essa pasta já existe")

  else: 
    data_atual = date.today()
    data_atual = data_atual.strftime('%d-%m-%Y')
    data_atual = str(data_atual)
    os.mkdir("Coleta_"+data_atual)
    print("Pasta secundária criada")
    os.chdir("Coleta_"+data_atual)
    #os.mkdir("Imagens_400px")
    #os.mkdir("Imagens_1417px")

def Coleta_Link(contador, id_index, startIndex):

  id_index = str(id_index)
  startIndex  = str(startIndex)

  link = 'https://science.mnhn.fr/institution/mnhn/collection/f/item/list?listCount=1&listIndex=1&startIndex='+startIndex
  url = requests.get(link)
  soup = BeautifulSoup(url.text, 'html.parser')

  icone = soup.find_all(class_="icon")
  for i in icone:
    lista_link.append(i.a)

  id_index = int(id_index)
  startIndex = int(startIndex)

def Formata_Palavra(contador, id_index): 
  link_item = lista_link[contador].get('href')

  return link_item

def coleta_info(id_index, contador, startIndex, n_registros): 
  #Definindo algumas variáveis
  data_atual = date.today()
  jpg = ".jpg"

  id = 1
  id = int(id)

  for i in range(n_registros):  

    id_index = str(id_index)

    id = str(id)

    print(id)
    

    link_item = Formata_Palavra(contador, id_index)
    link = 'https://science.mnhn.fr'+link_item
    print(link)
    url = requests.get(link)
    soup = BeautifulSoup(url.text, 'html.parser')
    titulo_pagina = soup.title.string

    if titulo_pagina == "Apache Tomcat/8.0.30 - Rapport d''erreur":
      id = int(id)
      id += 1

    elif titulo_pagina[0] == "I":
      id = int(id)
      id += 1

    else:

      #filo
      filo = soup.find(id="phylum")

      if filo is None:
        filo = "Não_informado"

      else:
        filo = soup.find(id="phylum").string

      #Classe
      classe = soup.find(id="class")
      if classe is None: 
        classe = "Não informado"

      else:
        classe = soup.find(id="class").string

      imagem = soup.find_all(class_='img-carousel')
      if imagem is None:
        foto = "Não informada"

      else:
        for img in imagem:
          if img.has_attr('src'):
            foto = img['src'] 
            print(foto)

            if os.path.exists(filo):
              os.chdir(filo)
              if os.path.exists(classe):
                os.chdir(classe)
                url_img_400 = foto
                response = requests.get(url_img_400)
                if response.status_code == 200:
                  with open(filo+"_"+classe+"_Img_"+id+jpg, 'wb') as f:
                    nome_foto = filo+"_"+classe+"_Img_"+id+jpg
                    f.write(response.content)
                print("Aqui agora: {}".format(classe))  
                os.chdir("/content/drive/My Drive/Projeto_IC_Unicamp/dados/Coletas/Coleta_"+data_atual)

              else:
                os.mkdir(classe)
                os.chdir(classe)
                url_img_400 = foto
                response = requests.get(url_img_400)
                if response.status_code == 200:
                  with open(filo+"_"+classe+"_Img_"+id+jpg, 'wb') as f:
                    nome_foto = filo+"_"+classe+"_Img_"+id+jpg
                    f.write(response.content)
                print("Aqui agora: {}".format(classe))
                os.chdir("/content/drive/My Drive/Projeto_IC_Unicamp/dados/Coletas/Coleta_"+data_atual)

            
            else:
              os.mkdir(filo)
              os.chdir(filo)
              os.mkdir(classe)
              os.chdir(classe)
              url_img_400 = foto
              response = requests.get(url_img_400)
              if response.status_code == 200:
                with open(filo+"_"+classe+"_Img_"+id+jpg, 'wb') as f:
                  nome_foto = filo+"_"+classe+"_Img_"+id+jpg
                  f.write(response.content)

              print("Aqui agora: {}".format(classe))
              #print(os.getcwd())
              #print(data_atual)
              data_atual = date.today()
              data_atual = data_atual.strftime('%d-%m-%Y')
              data_atual = str(data_atual)
              #print(data_atual)
              #print(type(data_atual))
              os.chdir("/content/drive/My Drive/Projeto_IC_Unicamp/dados/Coletas/Coleta_"+data_atual) 

      #Número do catálogo
      numero_catalogo = soup.find(class_='catalog-number-marked')
      if numero_catalogo is None:
        numero_catalogo = "Não informado"

      else:
        numero_catalogo = soup.find(class_='catalog-number-marked').string

      #Nome da Coleção original
      colecao_original = soup.find(class_='original-collection')
      if colecao_original is None:
        colecao_original = "Não informado"
      
      else:
        colecao_original = soup.find(class_='original-collection').a.string
        colecao_original = colecao_original.replace("\n", "")

      #Status da coleção
      estado_colecao = soup.find(id="occurrence-status")

      if estado_colecao is None:
        estado_colecao = "Não informado"

      else: 
        estado_colecao = soup.find(id="occurrence-status").string

      #classe
      classe = soup.find(id="class")
      
      if classe is None: 
        classe = "Não informado"

      else:
        classe = soup.find(id="class").string

      #ordem
      ordem = soup.find(id="order")
    
      if ordem is None:
        ordem = "Não informado"

      else:
        ordem = soup.find(id="order").string

      #familia
      familia = soup.find(id="family")

      if familia is None:
        familia = "Não informado"
      else:
        familia = soup.find(id="family").string

      #genero
      genero = soup.find(id="genus")

      if genero is None:
        genero = "Não informado"
      else:
        genero = soup.find(id="genus").string

      #especie
      especie = soup.find(id="species")

      if especie is None:
        especie = "Não informado"

      else:
        especie = soup.find(id="species").string

      #nome
      nome = soup.find(id="scientific-name")

      if nome is None:
        nome = "Não informado"
      else:
        nome = soup.find(id="scientific-name").text

      #País
      pais = soup.find(id="country")

      if pais is None:
        pais = "Não informado"
      else:
        pais = soup.find(id="country").text
        pais = pais.replace("\n", "")

      #Estado/Provincía
      estado_provincia = soup.find(id="stateProvince")

      if estado_provincia is None:
        estado_provincia = "Não informado"

      else: 
        estado_provincia = soup.find(id="stateProvince").string

      #Município
      municipio = soup.find(id="municipality")

      if municipio is None:
        municipio = "Não informado"

      else:
        municipio = soup.find(id="municipality").string

      #Nome coletor
      nome_coletor = soup.find(id="recordedBy")

      if nome_coletor is None:
        nome_coletor = "Não informado"

      else:
        nome_coletor = soup.find(id="recordedBy").a.text
        nome_coletor = nome_coletor.replace("\n", "")  

      #Estatigrafia
      ##Era
      era = soup.find(id="earliestEraOrLowestErathem")

      if era is None:
        era = "Não informado"

      else:
        era = soup.find(id="earliestEraOrLowestErathem").string

      #Sistema
      sistema = soup.find(id="earliestPeriodOrLowestSystem")

      if sistema is None:
        sistema = "Não informado"

      else: 
        sistema = soup.find(id="earliestPeriodOrLowestSystem").string

      #series
      series = soup.find(id="earliestEpochOrLowestSeries")

      if series is None:
        series = "Não informado"

      else:
        series = series = soup.find(id="earliestEpochOrLowestSeries").string

      #Estágio
      estagio = soup.find(id="earliestAgeOrLowestStage")

      if estagio is None:
        estagio = "Não informado"
      
      else:
        estagio = soup.find(id="earliestAgeOrLowestStage").string
      
      csv_writer.writerow([id, link ,titulo_pagina, foto, nome_foto, numero_catalogo, 
                          colecao_original, estado_colecao, filo, classe, ordem,
                          familia, genero, especie, nome, pais, estado_provincia,
                          municipio, nome_coletor, era, sistema, series, estagio, data_atual])
      
      #Iterando o contador
      id = int(id)
      id += 1
      id_index = int(id_index)
      id_index += 1
      contador += 1

      if contador % 500  == 0:
        startIndex += 500
        Coleta_Link(contador, id_index, startIndex)

  csv_file.close()

if __name__ == '__main__':
  Cria_Diretorios()

  """##Iniciando algumas váriavies"""

  id_index = 0
  contador = 0
  lista_link = []
  startIndex = 0

  Coleta_Link(contador, id_index, startIndex)

  """##Função para pegar o link do fóssil """

  """##Criando o CSV"""

  csv_file = open('Dados.csv', 'w', newline='')
  csv_writer = csv.writer(csv_file)
  csv_writer.writerow(['id', 'link', 'Titulo_pagina', 'foto', 'nome_foto', 'numero_catalogo', 'colecao_original','estado_colecao', 'filo', 'classe', 'ordem', 'familia', 'genero', 'especie', 'nome', 'país', 'estado_provincia', 'municipio', 'nome_coletor', 'era', 'sistema', 'series', 'estagio', 'data_coleta'])

  n_registros = int(input("Digite o número de Fósseis que deseja coletar: "))
  coleta_info(contador, id_index, startIndex, n_registros)